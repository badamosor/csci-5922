{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "arabic_seq2seq_trans.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "code",
        "id": "ysii6jiiGR0Y",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "uBbC_4etGR0x",
        "outputId": "f62322a7-77fb-44c0-8acd-df00b2d342b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "LoIMqshkGR1I",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "        \n",
        "    def addSentence(self, sentence):\n",
        "        #print(\"input:\", sentence)\n",
        "        tokenized_text = nltk.word_tokenize(sentence)\n",
        "\n",
        "        #print(\"tokenized:\", tokenized_text)\n",
        "        for word in tokenized_text:\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "tfhp78G8GR1l",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import codecs\n",
        "import csv\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    #lines = open('%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
        "#     lines = open('arabic.txt', encoding='utf-8')\n",
        "#         read().strip().split('\\n')\n",
        "#     print (lines)\n",
        "    \n",
        "\n",
        "    #FILENAME = \"arabic_words.csv\"\n",
        "    #FILENAME = \"arabic_sentences_4_short_medium.csv\"\n",
        "    FILENAME = \"arabic_sentences_4.csv\"\n",
        "\n",
        "    ENCODING = 'utf-8'\n",
        "    pairs = []\n",
        "    with codecs.open(FILENAME, \"r\", ENCODING) as fp:\n",
        "      reader = csv.reader(fp)\n",
        "      for rows in reader:\n",
        "        #print(\"Row:\", rows[0])\n",
        "        #ascii = normalizeString(row[0])\n",
        "        pairs.append(rows)\n",
        "        \n",
        "    input_lang = Lang(lang1)\n",
        "    output_lang = Lang(lang2)\n",
        "    \n",
        "    #Split into train and test\n",
        "   \n",
        "    #pairs = pairs[0:100]\n",
        "    train_pairs, test_pairs = train_test_split(pairs, test_size=0.2, random_state=42)\n",
        "    \n",
        "    return input_lang, output_lang, train_pairs, test_pairs\n",
        "#     # Split every line into pairs and normalize\n",
        "#     pairs = [[normalizeString(s) for s in l.split(',')] for l in lines]\n",
        "\n",
        "#     # Reverse pairs, make Lang instances\n",
        "#     if reverse:\n",
        "#         pairs = [list(reversed(p)) for p in pairs]\n",
        "#         input_lang = Lang(lang2)\n",
        "#         output_lang = Lang(lang1)\n",
        "#     else:\n",
        "#         input_lang = Lang(lang1)\n",
        "#         output_lang = Lang(lang2)\n",
        "\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "50GtKx2bGR1w"
      },
      "cell_type": "markdown",
      "source": [
        "Since there are a *lot* of example sentences and we want to train\n",
        "something quickly, we'll trim the data set to only relatively short and\n",
        "simple sentences. Here the maximum length is 10 words (that includes\n",
        "ending punctuation) and we're filtering to sentences that translate to\n",
        "the form \"I am\" or \"He is\" etc. (accounting for apostrophes replaced\n",
        "earlier).\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Oo76sGlnGR2F"
      },
      "cell_type": "markdown",
      "source": [
        "The full process for preparing the data is:\n",
        "\n",
        "-  Read text file and split into lines, split lines into pairs\n",
        "-  Normalize text, filter by length and content\n",
        "-  Make word lists from sentences in pairs\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "x0HJXkk3GR2L",
        "outputId": "0c97ca4a-ed92-4b5a-806a-720022e4d721",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "cell_type": "code",
      "source": [
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, train_pairs, test_pairs = readLangs(lang1, lang2, reverse)\n",
        "  \n",
        "    print(\"Read %s training sentence pairs\" % len(train_pairs))\n",
        "    #pairs = filterPairs(pairs)\n",
        "    #print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in train_pairs:\n",
        "        #print(\"printing pairs:\", pair[0], pair[1])\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    \n",
        "    ##!!!Adding new words in test to training words\n",
        "    \n",
        "    print(\"Read %s test sentence pairs\" % len(test_pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in test_pairs:\n",
        "        #print(\"printing pairs:\", pair[0], pair[1])\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    \n",
        "    return input_lang, output_lang, train_pairs, test_pairs\n",
        "\n",
        "\n",
        "input_lang, output_lang, train_pairs, test_pairs = prepareData('NoDiac', 'Diac', False)\n",
        "print(random.choice(train_pairs))\n",
        "print(random.choice(test_pairs))\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 80 training sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "NoDiac 244\n",
            "Diac 254\n",
            "Read 20 test sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "NoDiac 288\n",
            "Diac 301\n",
            "['والمدن والنسبة إليهما أعرابي', 'والمدن والنسبة إليهما أعرابيٌّ']\n",
            "['صلى الله عليه وسلم', 'صلى اللّه عليه وسلم']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "5saHNMBx-xpo",
        "outputId": "5f78aee3-d802-40d4-819a-6c6532537ee8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "#print(random.choice([input_lang.word2index.keys()]))\n",
        "#print(random.choice([output_lang.word2index.keys()]))\n",
        "print([input_lang.word2index])\n",
        "print([output_lang.word2index])\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'على': 2, 'الحجاج': 3, 'يوما': 4, 'فقال': 5, 'يتق': 6, 'أحدا': 7, 'قال': 8, 'الكميت': 9, 'منهم': 10, 'وأخذ': 11, 'من': 12, 'لفظه': 13, 'صلى': 14, 'الله': 15, 'عليه': 16, 'وسلم': 17, 'دارا': 18, 'وأحسنه': 19, 'جوارا': 20, 'وأعربه': 21, 'فيها': 22, 'كثير': 23, 'السقم': 24, 'وقد': 25, 'الأعراب': 26, 'آمنا': 27, 'قل': 28, 'لم': 29, 'أجرا': 30, 'إلا': 31, 'المودة': 32, 'خلاف': 33, 'العجم': 34, 'وهما': 35, 'واحد': 36, 'وجعل': 37, 'النبي': 38, 'الجوهري': 39, 'العريب': 40, 'تصغير': 41, 'العرب': 42, 'نعما': 43, 'ورعوا': 44, 'مساقط': 45, 'الغيث': 46, 'تعربا': 47, 'واستعرب': 48, 'استعرابا': 49, 'كل': 50, 'لائل': 51, 'تقول': 52, 'عرب': 53, 'عاربة': 54, 'وقال': 55, 'معرب': 56, 'مفصح': 57, 'الصحاح': 58, 'في': 59, 'اللغةالصحاح': 60, 'ترقرق': 61, 'مناكبها': 62, 'الدماء': 63, 'نهار': 64, 'ثم': 65, 'هي': 66, 'ثابتا': 67, 'وإن': 68, 'نلت': 69, 'منها': 70, 'كما': 71, 'نلتم': 72, 'بعدما': 73, 'كانوا': 74, 'عربا': 75, 'وفي': 76, 'لنبط': 77, 'وإنما': 78, 'اسم': 79, 'لسان': 80, 'والعرب': 81, 'للتقية': 82, 'الأزهري': 83, 'والخطاب': 84, 'فأكد': 85, 'به': 86, 'كقولك': 87, 'ليل': 88, 'كبيض': 89, 'الدجاج': 90, 'وبيض': 91, 'الجراد': 92, 'سمي': 93, 'الإعراب': 94, 'إعرابا': 95, 'لتبيينه': 96, 'العاربة': 97, 'هم': 98, 'الخلص': 99, 'عليهم': 100, 'عبدالمؤمن': 101, 'ابن': 102, 'عبدالقدوس': 103, 'فأما': 104, 'جذيلها': 105, 'المحكك': 106, 'وعذيقها': 107, 'المرجب': 108, 'كلمات': 109, 'أو': 110, 'تعابير': 111, 'مطابقة': 112, 'فلم': 113, 'أر': 114, 'كضب': 115, 'كان': 116, 'بدويا': 117, 'صاحب': 118, 'نجعة': 119, 'جيل': 120, 'الناس': 121, 'معروف': 122, 'البادية': 123, 'مقيما': 124, 'المصدر': 125, 'يكن': 126, 'فصيحا': 127, 'وجمعه': 128, 'حم': 129, 'آية': 130, 'تأولها': 131, 'منا': 132, 'أعرابيا': 133, 'والعربية': 134, 'هذه': 135, 'الراء': 136, 'كفصح': 137, 'وزنا': 138, 'ومعنى': 139, 'مستساغوالعذب': 140, 'عذر': 141, 'يعدونه': 142, 'المدينة': 143, 'طمعا': 144, 'الصدقات': 145, 'والمدن': 146, 'والنسبة': 147, 'إليهما': 148, 'أعرابي': 149, 'أباهم': 150, 'إسمعيل': 151, 'أشد': 152, 'كفرا': 153, 'ونفاقا': 154, 'الصديق': 155, 'رضي': 156, 'عنه': 157, 'اليمن': 158, 'وكانوا': 159, 'أهل': 160, 'عمد': 161, 'الكلام': 162, 'تعريبا': 163, 'وأعربت': 164, 'له': 165, 'رملا': 166, 'وزرود': 167, 'يقول': 168, 'أقام': 169, 'تؤمنوا': 170, 'ولكن': 171, 'قولوا': 172, 'أسلمنا': 173, 'أنطق': 174, 'لسانه': 175, 'بلغة': 176, 'تعربوا': 177, 'أي': 178, 'صاروا': 179, 'أعرابا': 180, 'إذا': 181, 'بينته': 182, 'سموا': 183, 'معهم': 184, 'ليس': 185, 'جمعا': 186, 'لعرب': 187, 'بالتفصيل': 188, 'وتقي': 189, 'ساكت': 190, 'عن': 191, 'باسم': 192, 'بلدهم': 193, 'أعرب': 194, 'الأعجمي': 195, 'وتعرب': 196, 'وعرباء': 197, 'صرحاء': 198, 'ومتعربة': 199, 'ومستعربة': 200, 'أن': 201, 'مهاجرا': 202, 'وكان': 203, 'طعام': 204, 'العريـ': 205, 'ـب': 206, 'لا': 207, 'للأجهزة': 208, 'المحمولة': 209, 'توجد': 210, 'تعظيما': 211, 'أنا': 212, 'وأعرب': 213, 'أحيانا': 214, 'بها': 215, 'فأصارح': 216, 'سيبويه': 217, 'كمكلم': 218, 'وأورد': 219, 'قوما': 220, 'وأفصح': 221, 'الأغتم': 222, 'إفصاحا': 223, 'مثله': 224, 'إله': 225, 'سبع': 226, 'شفاء': 227, 'القرم': 228, 'ومكن': 229, 'الضباب': 230, 'وصالح': 231, 'وهود': 232, 'صلوات': 233, 'عربيا': 234, 'البهط': 235, 'وحيتانكم': 236, 'فما': 237, 'زلت': 238, 'بالحق': 239, 'يتوقاهم': 240, 'وعربي': 241, 'الحديث': 242, 'ثلاث': 243, 'أحسابا': 244, 'أبينهم': 245, 'وأوضحهم': 246, 'العذب': 247, 'الشراب': 248, 'والطعام': 249, 'عز': 250, 'ومثلهم': 251, 'الذين': 252, 'ذكرهم': 253, 'فسماهم': 254, 'تعالى': 255, 'وجل': 256, 'قالت': 257, 'تشتهيه': 258, 'نفوس': 259, 'صغرهم': 260, 'مثل': 261, 'والعجم': 262, 'مؤنث': 263, 'no_diacritics': 264, 'هرم': 265, 'وما': 266, 'البيوض': 267, 'دخلاء': 268, 'ليسوا': 269, 'بخلص': 270, 'والعربي': 271, 'لأنه': 272, 'الأنباط': 273, 'تقي': 274, 'هكذا': 275, 'أنشده': 276, 'ذات': 277, 'صلة': 278, 'الباحث': 279, 'ولم': 280, 'يسموا': 281, 'وتقول': 282, 'الليث': 283, 'أبو': 284, 'الهندي': 285, 'واسمه': 286, 'والأعرابي': 287}]\n",
            "[{'على': 2, 'الحجاج': 3, 'يوماً': 4, 'فقال': 5, 'يتقِّ': 6, 'أحداً': 7, 'قال': 8, 'الكميت': 9, 'منهم': 10, 'وأُخِذ': 11, 'من': 12, 'لَفْظه': 13, 'صلى': 14, 'اللّه': 15, 'عليه': 16, 'وسلم': 17, 'داراً': 18, 'وأحسنه': 19, 'جواراً': 20, 'وأعربه': 21, 'فيها': 22, 'كثيرَ': 23, 'السَّقَمْ': 24, 'وقد': 25, 'الأَعْرَابُ': 26, 'آمَنَّا': 27, 'قُل': 28, 'لَّمْ': 29, 'أجراً': 30, 'إلا': 31, 'المودة': 32, 'خِلافُ': 33, 'العَجَم': 34, 'وهما': 35, 'واحدٌ': 36, 'وجعل': 37, 'النبي': 38, 'الجوهري': 39, 'العُرَيْبُ': 40, 'تصغير': 41, 'العَرَبِ': 42, 'نعماً': 43, 'ورعوا': 44, 'مساقط': 45, 'الغيث': 46, 'تعرباً': 47, 'واستعرب': 48, 'استعراباً': 49, 'كل': 50, 'لائِلٌ': 51, 'تقول': 52, 'عَرَبٌ': 53, 'عارِبةٌ': 54, 'وقال': 55, 'معربٌ': 56, 'مفصحٌ': 57, 'الصّحّاح': 58, 'في': 59, 'اللغةالصّحّاح': 60, 'ترقرق': 61, 'مناكبها': 62, 'الدماءُ': 63, 'نهارٍ': 64, 'ثم': 65, 'هي': 66, 'العرب': 67, 'ثابتاً': 68, 'وإن': 69, 'لم': 70, 'نِلْتُ': 71, 'منها': 72, 'كما': 73, 'نِلْتُمُ': 74, 'بعدما': 75, 'كانوا': 76, 'عرباً': 77, 'وفي': 78, 'لنبطٍ': 79, 'وإنما': 80, 'اسم': 81, 'لسان': 82, 'العُرْبُ': 83, 'والعَرَبُ': 84, 'للتقيّة': 85, 'الأزهري': 86, 'والخطاب': 87, 'فأُكِّدَ': 88, 'به': 89, 'كقولك': 90, 'لَيلٌ': 91, 'كبَيْضِ': 92, 'الدَّجاج': 93, 'وبَيْضُ': 94, 'الجَرادِ': 95, 'سمي': 96, 'الإعراب': 97, 'إعراباً': 98, 'لتبيينه': 99, 'العارِبة': 100, 'هم': 101, 'الخُلَّصُ': 102, 'عليهم': 103, 'عَبْدُالمؤمن': 104, 'ابن': 105, 'عبدالقُدُوس': 106, 'فأَمَّا': 107, 'جُذَيْلُها': 108, 'المُحَكَّكُ': 109, 'وعُذَيْقُها': 110, 'المُرَجَّبُ': 111, 'كَلِمَاتٌ': 112, 'أو': 113, 'تَعَابِيرٌ': 114, 'مُطَابِقَةٌ': 115, 'فلَمْ': 116, 'أرَ': 117, 'كَضَبٍّ': 118, 'كان': 119, 'بدوياً': 120, 'صاحب': 121, 'نجعة': 122, 'جِيْلٌ': 123, 'الناس': 124, 'معروف': 125, 'البادية': 126, 'مقيماً': 127, 'المصدر': 128, 'عرب': 129, 'يكن': 130, 'فصيحاً': 131, 'وجمعه': 132, 'حمِ': 133, 'آية': 134, 'تأولها': 135, 'منا': 136, 'أعرابياً': 137, 'والعربية': 138, 'هذه': 139, 'الراء': 140, 'كفصح': 141, 'وزناً': 142, 'ومعنى': 143, 'كُلُّ': 144, 'مُسْتَسَاغٍوالعَذْبُ': 145, 'عذر': 146, 'يعدونه': 147, 'المدينة': 148, 'طمعاً': 149, 'الصدقات': 150, 'والمدن': 151, 'والنسبة': 152, 'إليهما': 153, 'أعرابيٌّ': 154, 'أباهم': 155, 'إسمعيل': 156, 'الأَعْرابُ': 157, 'أَشدّ': 158, 'كُفراً': 159, 'ونِفاقاً': 160, 'الصديق': 161, 'رضي': 162, 'عنه': 163, 'اليمن': 164, 'وكانوا': 165, 'أهل': 166, 'عمدٍ': 167, 'الكلام': 168, 'تعريباً': 169, 'وأعربت': 170, 'له': 171, 'رملاً': 172, 'وزرود': 173, 'يقول': 174, 'أقام': 175, 'تُؤْمِنُوا': 176, 'وَلَكِن': 177, 'قُولُوا': 178, 'أَسْلَمْنَا': 179, 'أنطق': 180, 'لسانه': 181, 'بلغة': 182, 'تعربوا': 183, 'أي': 184, 'صاروا': 185, 'أعراباً': 186, 'إذا': 187, 'بينته': 188, 'سموا': 189, 'معهم': 190, 'ليس': 191, 'الأعراب': 192, 'جمعاً': 193, 'لعرب': 194, 'بالتفصيل': 195, 'وتقيٌّ': 196, 'ساكتٌ': 197, 'عنهُ': 198, 'عن': 199, 'باسم': 200, 'بلدهم': 201, 'أعرب': 202, 'الأعجمي': 203, 'وتعرب': 204, 'معرب': 205, 'وعَرْباءُ': 206, 'صُرَحاءُ': 207, 'ومُتَعَرِّبةُ': 208, 'ومُسْتَعْرِبةٌ': 209, 'أن': 210, 'مهاجراً': 211, 'وكان': 212, 'طَعامُ': 213, 'العُرَيْـ': 214, 'ـبِ': 215, 'لا': 216, 'للأجهزة': 217, 'المحمولة': 218, 'تُوجَدُ': 219, 'تعظيماً': 220, 'أنا': 221, 'وأعرب': 222, 'أحياناً': 223, 'بها': 224, 'فأصارح': 225, 'سيبويه': 226, 'كَمُكَلِّمٍ': 227, 'وأورد': 228, 'قوماً': 229, 'وأفصح': 230, 'الأغتم': 231, 'إفصاحاً': 232, 'مثله': 233, 'إله': 234, 'سبع': 235, 'شِفاءُ': 236, 'القَرِمْ': 237, 'ومَكْنُ': 238, 'الضِّبابِ': 239, 'وصالح': 240, 'وهود': 241, 'صلوات': 242, 'عربياً': 243, 'البَهَطُ': 244, 'وحِيتَانُكُم': 245, 'فما': 246, 'زِلْتُ': 247, 'مفصح': 248, 'بالحقِّ': 249, 'يتوقاهم': 250, 'وعربيٌّ': 251, 'الحديث': 252, 'ثلاث': 253, 'أحساباً': 254, 'أبينهم': 255, 'وأوضحهم': 256, 'العَذْبُ': 257, 'الشَّرابِ': 258, 'والطَّعَامِ': 259, 'عز': 260, 'ومثلْهم': 261, 'الذين': 262, 'ذكرهم': 263, 'فسماهم': 264, 'تعالى': 265, 'وجل': 266, 'قَالَتِ': 267, 'تَشْتَهيهِ': 268, 'نفوسُ': 269, 'العَجَمْ': 270, 'صَغَّرهم': 271, 'مثل': 272, 'العُجْمِ': 273, 'والعَجَم': 274, 'مؤنث': 275, 'with_diacritics': 276, 'هَرِمْ': 277, 'وما': 278, 'البُيُوضِ': 279, 'دُخَلاءُ': 280, 'ليسوا': 281, 'بخُلَّصٍ': 282, 'والعربي': 283, 'لأنه': 284, 'الأنباط': 285, 'تقيٌّ': 286, 'معربُ': 287, 'هكذا': 288, 'أنشده': 289, 'ذَاتُ': 290, 'صِلَة': 291, 'الباحث': 292, 'ولم': 293, 'يسموا': 294, 'وتقول': 295, 'الليث': 296, 'أبو': 297, 'الهِنْدِيّ': 298, 'واسمه': 299, 'والأعرابي': 300}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "N340KZrDGR2b",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "YVXi0PIMGR2q",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "06B2-qPlv-4B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "p4m3jSfWGR25",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "       "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "t4oZV5bkGR3G",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "      return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    #print(\"Pair is:\",pair[0],pair[1])\n",
        "    #print(\"Tensors:\",input_tensor, target_tensor)\n",
        "    return (input_tensor, target_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "PHt1EuhYGR3W",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "    #print(\"input_length\",input_length,\"target_length:\",target_length)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    #print(encoder_outputs.shape)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    #use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "    use_teacher_forcing = False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            #decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            #decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "4_ei9uAkGR3i"
      },
      "cell_type": "markdown",
      "source": [
        "This is a helper function to print time elapsed and estimated time\n",
        "remaining given the current time and progress %.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Y8A54Vp4GR3m",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "TcUMZE5ZGR33"
      },
      "cell_type": "markdown",
      "source": [
        "The whole training process looks like this:\n",
        "\n",
        "-  Start a timer\n",
        "-  Initialize optimizers and criterion\n",
        "-  Create set of training pairs\n",
        "-  Start empty losses array for plotting\n",
        "\n",
        "Then we call ``train`` many times and occasionally print the progress (%\n",
        "of examples, time so far, estimated time) and average loss.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "K0gOI3bTGR39",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    #training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "     #                 for i in range(n_iters)]\n",
        "    training_pairs = [tensorsFromPair(random.choice(train_pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        #print(\"Training pair\", training_pair)\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    #print (\"got here\")\n",
        "    showPlot(plot_losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "lS_prqlNGR4I"
      },
      "cell_type": "markdown",
      "source": [
        "Plotting results\n",
        "----------------\n",
        "\n",
        "Plotting is done with matplotlib, using the array of loss values\n",
        "``plot_losses`` saved while training.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "1up_xUiDGR4M",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "VQTN2PemGR4U"
      },
      "cell_type": "markdown",
      "source": [
        "Evaluation\n",
        "==========\n",
        "\n",
        "Evaluation is mostly the same as training, but there are no targets so\n",
        "we simply feed the decoder's predictions back to itself for each step.\n",
        "Every time it predicts a word we add it to the output string, and if it\n",
        "predicts the EOS token we stop there. We also store the decoder's\n",
        "attention outputs for display later.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "mL0VFrDkGR4X",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "           # decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "           # decoder_attentions[di] = decoder_attention.data\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)            \n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                #decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                #if (topi.item() not in output_lang.test_index2word):\n",
        "                  #decoded_words.append(\"unk\")\n",
        "                #else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        #return decoded_words, decoder_attentions[:di + 1]\n",
        "        return decoded_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "MCcY-9C-GR4g"
      },
      "cell_type": "markdown",
      "source": [
        "We can evaluate random sentences from the training set and print out the\n",
        "input, target, and output to make some subjective quality judgements:\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "VUWzSXGTGR4j",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    correct = 0\n",
        "    incorrect = 0\n",
        "    for i in range(n):\n",
        "        #pair = random.choice(pairs)\n",
        "        pair = random.choice(test_pairs)\n",
        "\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        #output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_words = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')\n",
        "        \n",
        "        tokenized_target = nltk.word_tokenize(pair[1])\n",
        "        \n",
        "        for target, output in zip(tokenized_target, output_words):\n",
        "          if (target == output):\n",
        "            correct += 1\n",
        "          else:\n",
        "            incorrect += 1\n",
        "            print(\"Example incorrect:\")\n",
        "            print(target, output)\n",
        "                \n",
        "#        if (pair[1] == output_sentence):\n",
        "#           correct += 1\n",
        "#         else:\n",
        "#           print(\"Example incorrect:\")\n",
        "#           incorrect += 0\n",
        "    print(\"accuracy:\",correct/(correct+incorrect),correct,incorrect) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "iixQL-KaEXHd",
        "outputId": "58fc1797-3516-4008-d058-42fabd184f8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"input:\", input_lang.n_words)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input: 288\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "7ARNE0-BGR43",
        "outputId": "06dc9f78-e8ea-4e59-d02d-1c4378573d85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        }
      },
      "cell_type": "code",
      "source": [
        "hidden_size = 256\n",
        "\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "#attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "decoder1 = DecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "#print(\"Embeddings:\", encoder1.embedding)\n",
        "\n",
        "#trainIters(encoder1, attn_decoder1, 75000, print_every=5000)\n",
        "#trainIters(encoder1, decoder1, 1000, print_every=500)\n",
        "trainIters(encoder1, decoder1, 20000, print_every=500)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0m 11s (- 7m 21s) (500 2%) 4.0195\n",
            "0m 21s (- 6m 43s) (1000 5%) 3.4113\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-feb94ff21fb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#trainIters(encoder1, attn_decoder1, 75000, print_every=5000)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#trainIters(encoder1, decoder1, 1000, print_every=500)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrainIters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-46-d05e6b6043a5>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(encoder, decoder, n_iters, print_every, plot_every, learning_rate)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mtarget_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_pair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mprint_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mplot_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-de8fc7d87939>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m                         \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "IfawPGi9GR4_",
        "outputId": "c56a6a56-d48e-43c1-a0ae-f334987951a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1700
        }
      },
      "cell_type": "code",
      "source": [
        "evaluateRandomly(encoder1, decoder1)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> ولم يسموا أعرابا وتقول\n",
            "= ولم يسموا أعراباً وتقول\n",
            "< بالتفصيل كان كان\n",
            "\n",
            "Example incorrect:\n",
            "ولم بالتفصيل\n",
            "Example incorrect:\n",
            "يسموا كان\n",
            "Example incorrect:\n",
            "أعراباً كان\n",
            "> أحسابا أي أبينهم وأوضحهم\n",
            "= أحساباً أي أبينهم وأوضحهم\n",
            "< للتقيّة قال الأزهري\n",
            "\n",
            "Example incorrect:\n",
            "أحساباً للتقيّة\n",
            "Example incorrect:\n",
            "أي قال\n",
            "Example incorrect:\n",
            "أبينهم الأزهري\n",
            "> كان فصيحا وقال الليث\n",
            "= كان فصيحاً وقال الليث\n",
            "< للتقيّة كان كان\n",
            "\n",
            "Example incorrect:\n",
            "كان للتقيّة\n",
            "Example incorrect:\n",
            "فصيحاً كان\n",
            "Example incorrect:\n",
            "وقال كان\n",
            "> أحسابا أي أبينهم وأوضحهم\n",
            "= أحساباً أي أبينهم وأوضحهم\n",
            "< للتقيّة قال الأزهري\n",
            "\n",
            "Example incorrect:\n",
            "أحساباً للتقيّة\n",
            "Example incorrect:\n",
            "أي قال\n",
            "Example incorrect:\n",
            "أبينهم الأزهري\n",
            "> لم يكن بدويا والأعرابي\n",
            "= لم يكن بدوياً والأعرابي\n",
            "< للتقيّة قال الأزهري\n",
            "\n",
            "Example incorrect:\n",
            "لم للتقيّة\n",
            "Example incorrect:\n",
            "يكن قال\n",
            "Example incorrect:\n",
            "بدوياً الأزهري\n",
            "> أو ذات صلة الباحث\n",
            "= أو ذَاتُ صِلَة الباحث\n",
            "< للتقيّة قال الأزهري\n",
            "\n",
            "Example incorrect:\n",
            "أو للتقيّة\n",
            "Example incorrect:\n",
            "ذَاتُ قال\n",
            "Example incorrect:\n",
            "صِلَة الأزهري\n",
            "> هرم وما في البيوض\n",
            "= هَرِمْ وما في البُيُوضِ\n",
            "< المدينة في في\n",
            "\n",
            "Example incorrect:\n",
            "هَرِمْ المدينة\n",
            "Example incorrect:\n",
            "وما في\n",
            "> ومثلهم الذين ذكرهم الله\n",
            "= ومثلْهم الذين ذكرهم اللّه\n",
            "< أباهم النبي اللّه\n",
            "\n",
            "Example incorrect:\n",
            "ومثلْهم أباهم\n",
            "Example incorrect:\n",
            "الذين النبي\n",
            "Example incorrect:\n",
            "ذكرهم اللّه\n",
            "> كما كان الأنباط جمعا\n",
            "= كما كان الأنباط جمعاً\n",
            "< البادية بعدما كان\n",
            "\n",
            "Example incorrect:\n",
            "كما البادية\n",
            "Example incorrect:\n",
            "كان بعدما\n",
            "Example incorrect:\n",
            "الأنباط كان\n",
            "> ولم يسموا أعرابا وتقول\n",
            "= ولم يسموا أعراباً وتقول\n",
            "< بالتفصيل كان كان\n",
            "\n",
            "Example incorrect:\n",
            "ولم بالتفصيل\n",
            "Example incorrect:\n",
            "يسموا كان\n",
            "Example incorrect:\n",
            "أعراباً كان\n",
            "accuracy: 0.03333333333333333 1 29\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "RJ6mO4BZGR5G"
      },
      "cell_type": "markdown",
      "source": [
        "Visualizing Attention\n",
        "---------------------\n",
        "\n",
        "A useful property of the attention mechanism is its highly interpretable\n",
        "outputs. Because it is used to weight specific encoder outputs of the\n",
        "input sequence, we can imagine looking where the network is focused most\n",
        "at each time step.\n",
        "\n",
        "You could simply run ``plt.matshow(attentions)`` to see attention output\n",
        "displayed as a matrix, with the columns being input steps and rows being\n",
        "output steps:\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "vlvD1dFoGR5J",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "output_words, attentions = evaluate(\n",
        "    encoder1, attn_decoder1, \"je suis trop froid .\")\n",
        "plt.matshow(attentions.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "kpL-VgNrGR5T"
      },
      "cell_type": "markdown",
      "source": [
        "For a better viewing experience we will do the extra work of adding axes\n",
        "and labels:\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Eu_Lwt8IGR5a",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def showAttention(input_sentence, output_words, attentions):\n",
        "    # Set up figure with colorbar\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
        "                       ['<EOS>'], rotation=90)\n",
        "    ax.set_yticklabels([''] + output_words)\n",
        "\n",
        "    # Show label at every tick\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def evaluateAndShowAttention(input_sentence):\n",
        "    output_words, attentions = evaluate(\n",
        "        encoder1, attn_decoder1, input_sentence)\n",
        "    print('input =', input_sentence)\n",
        "    print('output =', ' '.join(output_words))\n",
        "    showAttention(input_sentence, output_words, attentions)\n",
        "\n",
        "\n",
        "evaluateAndShowAttention(\"elle a cinq ans de moins que moi .\")\n",
        "\n",
        "evaluateAndShowAttention(\"elle est trop petit .\")\n",
        "\n",
        "evaluateAndShowAttention(\"je ne crains pas de mourir .\")\n",
        "\n",
        "evaluateAndShowAttention(\"c est un jeune directeur plein de talent .\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}